{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Condenser","text":"<p>Condenser is a private cloud platform for UCL. Based on traditional High-Performance Computing (HPC) hardware, the platform uses virtualisation and containerisation technologies to support a variety of services- from web applications to HPC clusters and Trusted Research Environments (TREs). Condenser is designed to enable cost-effective, cloud-native research computing at UCL.</p> <p>Condenser is developed and maintained by the Environments team within the Advanced Research Computing Centre. Users can refer to our Documentation for developers for advice about deploying virtual resources on Condenser.</p>"},{"location":"#by-the-numbers","title":"By the numbers","text":"<p>Condenser comprises a large pool of computing resources, including:</p> <ul> <li>8160 cores</li> <li>85TB RAM</li> <li>25 GbE interconnect</li> <li>A100, H100, and H200 GPUs</li> </ul>"},{"location":"#technology-stack","title":"Technology stack","text":"<p>Condenser is developed using Infrastructure as Code (IaC) and Continuous Integration/Continuous Deployment (CI/CD) methodologies. Some of the technologies used to build the platform are:</p> Edge Fortinet Platform Rancher,     Harvester,     K3s Deployment Terraform,     Packer Hardware Lenovo,     Nvidia,     IBM,"},{"location":"about/","title":"About the Platform","text":"<p>Condenser is a private cloud platform for research computing at UCL. It provides a cost-effective, private alternative to public cloud platforms. The platform is developed and maintained by the Advanced Research Computing Centre. All hardware resides in UCL facilities.</p>"},{"location":"about/#tenancy","title":"Tenancy","text":"<p>Access to Condenser is organized according to tenancies. Users in a tenancy are provided with access to a Kubernetes cluster. Users can deploy virtual machines and related resources with Harvester. Rancher is used to provide access to the cluster.</p> <p>A tenancy is provided with a namespace to isolate their resources from neighbor tenants on the same cluster. Each tenancy is also provided with an isolated network. Resource quotas (CPU, RAM, and data storage) are applied at the tenancy level, rather than to individual users.</p> <p></p>"},{"location":"about/#sensitive-data","title":"Sensitive data","text":"<p>As a platform, Condenser is not certified for the secure storage and processing of sensitive data. However, it is possible to build a secure environment on the platform. Please consult with us during onboarding, and plan to collaborate with a Research Data Engineer when planning projects with sensitive data on Condenser.</p>"},{"location":"prospective_user_guidance/","title":"Prospective user guidance","text":"<p>Condenser is a private cloud alternative to public cloud offerings. In many cases, Condenser can provide a superior experience for UCL research and at a lower cost. We have written this guidance to help prospective users decide whether their project can run on the Condenser platform. This guidance is not a binding contract.</p> <p>To discuss your potential tenancy and obtain a quote, please create a request in MyServices under the \"Condenser\" service.</p>"},{"location":"prospective_user_guidance/#our-responsibilities","title":"Our responsibilities","text":"<p>Condenser is supported by the ARC Environments and Infrastructure teams. We maintain the hardware and virtual infrastructure for the clusters in Condenser. We are responsible for:</p> <ul> <li>Updating the platform software</li> <li>Implementing platform-level security measures, such as role-based access control,   tenant isolation, and SSH certificate registration</li> <li>Maintaining access from the platform to the internet</li> <li>Making up-to-date OS images available on the platform</li> <li>Maintaining platform features such as SSH access and web ingress for virtual machines</li> <li>Creating and updating documentation for the Condenser platform</li> <li>Onboarding new users</li> <li>Provisioning new tenancies</li> <li>Implementing and amending tenancy quotas</li> <li>Maintaining the clusters to a standard such that users can deploy virtual machines,   data volumes, networks, and other resources within their tenancy quota</li> <li>Supporting users in their access to and normal use of the platform through UCL   MyServices</li> </ul>"},{"location":"prospective_user_guidance/#developing-on-condenser","title":"Developing on Condenser","text":"<p>Condenser provides a platform for your team to build on. To enable your project to succeed on Condenser, please ensure that your team includes staff with the appropriate skills and time to support the project. Depending on the nature of your project, your team may need to be able to do the following tasks:</p> <ul> <li>Developing and configuring your project's application software</li> <li>Deploying virtual machines, data volumes, and networks (your virtual resources)   for your application</li> <li>Configuring your virtual resources to make use of platform features such as SSH   access and web ingress</li> <li>Using up-to-date operating systems for your virtual machines</li> <li>Configuring security features in your virtual resources and application software,   such as configuring the firewall in your virtual machines</li> <li>Securing your environment to professional standards for the storage and   processing of sensitive data</li> <li>Keeping your data safe by using external version control and backups</li> <li>Supporting third-party users in their access to and use of your application</li> </ul>"},{"location":"terms_and_conditions/","title":"Terms and Conditions of Use","text":"<p>All use of the Condenser platform is subject to the general UCL Computing Regulations.</p> <p>The following terms also apply to the use of Condenser.</p> <p>Condenser is a private cloud platform on which you may deploy virtual computing resources such as data volumes and virtual machines. In this document, we refer to these as your resources.</p> <p>The terms of service on Condenser will be described by a forthcoming User Guidance and Responsibilities document.</p>"},{"location":"terms_and_conditions/#commercial-services","title":"Commercial Services","text":"<p>It is not permitted to provide commercial services from Condenser.</p>"},{"location":"terms_and_conditions/#access-suspension","title":"Access Suspension","text":"<p>If you</p> <ul> <li>are subject to a UCL disciplinary procedure,</li> <li>or breach UCL's Computing Regulations,</li> <li>or are suspected to have shared your access credentials,</li> <li>or are suspected to be involved in a security incident,</li> </ul> <p>we may raise this with relevant UCL teams, such as the Information Security Group, or Human Resources. We may suspend your access to the platform until any relevant processes have been completed.</p>"},{"location":"terms_and_conditions/#your-resources-and-data","title":"Your Resources and Data","text":"<p>Your resources on Condenser should not be treated as the sole repository for your data. We do not make contractual agreements about uptime or availability.</p> <p>Outages and data-loss events will be handled on a best-efforts basis within normal UCL working hours.</p>"},{"location":"terms_and_conditions/#resource-access-by-arc-staff","title":"Resource Access by ARC Staff","text":"<p>We will not access or change your resources without your consent, except in the following circumstances:</p> <ul> <li>We require access to your resource to resolve a technical issue.</li> <li>We have reason to believe your resource is relevant to investigating an on-going security incident.</li> <li>In automated services, such as transfers between storage systems for service maintenance.</li> </ul>"},{"location":"terms_and_conditions/#acknowledgement-in-works","title":"Acknowledgement in Works","text":"<p>We request that you acknowledge the use of the Condenser platform in any publications describing research that was accomplished using your resources on Condenser. We suggest the following acknowledgement:</p> <p>\"The authors acknowledge the use of the UCL Condenser private cloud platform, and the support of the UCL Centre for Advanced Research Computing, in the completion of this work.\"</p>"},{"location":"documentation/","title":"Getting Started","text":"<p>The Condenser platform provides pools of compute and storage resources that can be used to create virtual machines, networks, and volumes.</p>"},{"location":"documentation/#deploying-virtual-resources","title":"Deploying virtual resources","text":"<p>Within the UCL network, Condenser can be accessed through Rancher. Resources can be deployed manually through this interface. Resources can also be automatically deployed using Infrastructure as Code (IaC) methods such as Terraform.</p> <p>We have documentation demonstrating a simple Terraform deployment and the equivalent manual deployment in the Rancher GUI.</p> <p>Some advantages of describing deployments with IaC are that deployments become repeatable and are easier to audit and contribute to. We collect examples of deployments for Condenser.</p> <p>Both the GUI and Terraform deployments use Harvester to deploy resources on the underlying kubernetes cluster, which uses kubevirt to provide virtual machines. In case a deployment requires features that are not yet enabled by Harvester, resources can also be deployed directly onto a kubernetes cluster with <code>kubectl</code>.</p>"},{"location":"documentation/#administration-of-virtual-resources","title":"Administration of virtual resources","text":"<p>On Condenser, developers are responsible for administering their own virtual resources. Please refer to the appropriate documentation. Some starting points are listed below.</p>"},{"location":"documentation/#operating-systems","title":"Operating systems","text":"<ul> <li>Red Hat Enterprise Linux 9</li> <li>Almalinux</li> <li>Ubuntu</li> <li>Debian</li> </ul>"},{"location":"documentation/#cloud-infrastructure","title":"Cloud infrastructure","text":"<ul> <li>Harvester</li> <li>Cloudinit</li> <li>Kubernetes</li> </ul>"},{"location":"documentation/examples/","title":"Examples","text":""},{"location":"documentation/examples/#modules","title":"Modules","text":"<p>These repositories contain modules that have been developed for use in deployments on Condenser.</p>"},{"location":"documentation/examples/#ucl-arcterraform-harvester-modules","title":"UCL-ARC/terraform-harvester-modules","text":"<p>This repository contains several useful modules for deploying virtual machines and k3s-based kubernetes clusters.</p>"},{"location":"documentation/examples/#deployments","title":"Deployments","text":"<p>These repositories describe complete deployments for Condenser.</p>"},{"location":"documentation/examples/#ucl-arcterraform-harvester-vm-demo","title":"UCL-ARC/terraform-harvester-vm-demo","text":"<p>A basic VM configured for SSH access.</p>"},{"location":"documentation/ssh_access/","title":"Configuring a VM for SSH access","text":"<p>To be accessible by SSH, a VM needs to be configured with a VLAN Network and the SSH public key data needs to be provided via cloudinit. We have documentation demonstrating this for Terraform deployments and for the Rancher GUI.</p> <p>After the VM is configured, the IP address for the VM can be found in the Rancher GUI or by using <code>kubectl</code> with a suitable kubeconfig file and the name of the namespace where the VM is deployed:</p> <pre><code>kubectl get virtualmachineinstance.kubevirt.io --namespace my-ns\n</code></pre> <p>The VM's base image will typically be configured with a default username with SSH login enabled. The username varies depending on the base image of the VM.</p> Image OS Default SSH username RHEL <code>cloud-user</code> Almalinux <code>almalinux</code> Ubuntu <code>ubuntu</code> Debian <code>debian</code> <p>VMs are not typically configured to allow SSH login to the root account.</p> <p>Follow these instructions to access a VM with SSH.</p>"},{"location":"documentation/ssh_login/","title":"Logging into a VM by SSH","text":"<p>All SSH access to Condenser must be routed through our SSH bastion, <code>ssh.condenser.arc.ucl.ac.uk</code>. This bastion does not act like a traditional login node, and instead only allows jumping (with the use of <code>ProxyJump</code> or <code>-J</code>) to another host on Condenser.</p> <p>To access SSH, you will need to have setup MFA on your UCL account. Both options below have enhanced MFA requirements, so you may be prompted more often than usual.</p>"},{"location":"documentation/ssh_login/#getting-a-certificate","title":"Getting a Certificate","text":""},{"location":"documentation/ssh_login/#web-interface","title":"Web Interface","text":"<p>You can quickly generate a new certificate without using the command line. The Portal will generate a new key and signed certificate. This certificate will be valid for 7 days. You can only have one active certificate at a time using this method. If you need multiple certificates, please use the CLI option below.</p> <ol> <li>Connect to the UCL VPN</li> <li>Go to the SSH Portal</li> <li>Sign in with your UCL credentials (this should happen automatically)</li> <li>Click \"Generate\" at the bottom of the page. If you see an \"Active Key\" message, click \"Revoke\" to revoke it. You can then generate a new one.</li> </ol>"},{"location":"documentation/ssh_login/#cli","title":"CLI","text":"<p>You can obtain a certificate directly through the CLI using the Vault client. You will need to provide your own (existing) SSH key to sign. On Windows, use PuttyGen to create a key. On Linux/Mac OS WSL, you can use:</p> <pre><code>ssh-keygen -t ed25519\n</code></pre> <p>To generate a certificate:</p> <ol> <li>Install the Vault client</li> <li>Set the <code>VAULT_ADDR</code> environment variable to <code>https://vault.arc.ucl.ac.uk</code> (you may wish to add this to your local environment)</li> <li>Connect to the ISD VPN</li> <li>Login to Vault:</li> </ol> <pre><code>vault login -method=oidc\n</code></pre> <ol> <li>Generate a certificate:</li> </ol> <pre><code>vault write -field=signed_key ssh-environments-ingress/sign/cloud-user \\\npublic_key=@/home/user/.ssh/id_ed25519.pub &gt;  ~/.ssh/id_arc.signed\n</code></pre> <p>(change <code>/home/user/.ssh/id_ed25519.pub</code> to the location of your own public key)</p>"},{"location":"documentation/ssh_login/#connecting","title":"Connecting","text":""},{"location":"documentation/ssh_login/#linuxmac-oswsl","title":"Linux/Mac OS/WSL","text":"<ol> <li> <p>Add a Host to your <code>~/.ssh/config</code>. If you generated the certificate using the SSH Portal, add:</p> <pre><code>Host condenser\n  HostName ssh.condenser.arc.ucl.ac.uk\n  User cloud-user\n  CertificateFile ~/.ssh/id_arc.signed\n  IdentityFile ~/.ssh/id_arc\n</code></pre> <p>If you generated your certificate using Vault directly with your own key, add:</p> <pre><code>Host condenser\n  HostName ssh.condenser.arc.ucl.ac.uk\n  User cloud-user\n  CertificateFile ~/.ssh/id_arc.signed\n  IdentityFile ~/.ssh/id_ed25519\n</code></pre> <p>Ensure that <code>CertificateFile</code> matches the filename you saved your certificate as in [5] above, and <code>IdentityFile</code> is the private key matching the public key provided to Vault.</p> </li> <li> <p>Connect to the UCL VPN</p> </li> <li> <p>Connect via SSH to your machine running on Condenser using <code>-J condenser</code>:</p> <pre><code>ssh -J condenser username@10.134.X.X\n</code></pre> <p>Contact the developer responsible for the VM for the correct username and IP address.</p> </li> </ol> <p>Note</p> <p>To avoid having to use <code>-J condenser</code> every time, you can define a ProxyJump in your <code>~/.ssh/config</code>. For example, if your servers are <code>*.your-subnet.condenser.ucl.ac.uk</code>, add the following:</p> <pre><code>Host *.your-subnet.condenser.arc.ucl.ac.uk\n  ProxyJump condenser\n</code></pre> <p>To always jump through the bastion for these hosts.</p>"},{"location":"documentation/web_ingress/","title":"Configuring a VM for web ingress","text":"<p>Web ingress to Condenser can be configured automatically by tagging resources. Ingress must be enabled on your tenancy before it can be configured.</p> <p>HTTPS ingress will be configured with:</p> <ul> <li>A URL: <code>https://[hostname].[rancher project name].condenser.arc.ucl.ac.uk</code></li> <li>A valid LetsEncrypt certificate</li> </ul> <p>By default traffic will be routed to the <code>eth0</code> network interface on the VM, using HTTP on port 80.</p> <p>If a VM's IP address changes, the ingress rule will be updated. If a VM is powered off, the ingress rule will be deleted. Once the VM is powered back on, the ingress rule will be recreated.</p> <p>Note</p> <p>Virtual machines can be configured with both tags and labels. On Condenser, ingress works by parsing tags into labels. This was done so that ingress can be configured automatically through Terraform, since only tags can be configured with the <code>harvester_virtualmachine</code> resource. In the GUI, you can configure either. You may wish to stick to one or the other (e.g. only modify tags, or only modify labels) to prevent confusion.</p>"},{"location":"documentation/web_ingress/#configuration","title":"Configuration","text":""},{"location":"documentation/web_ingress/#using-terraform","title":"Using Terraform","text":"<p>If using the Harvester Terraform provider, ingress rules should be configured using the <code>tags</code> argument on the <code>harvester_virtualmachine</code> resource.</p>"},{"location":"documentation/web_ingress/#enable-ingress-to-a-vm","title":"Enable Ingress to a VM","text":"<p>Add the following tag to enable ingress to a VM:</p> <pre><code>condenser_ingress_isEnabled: true\n</code></pre>"},{"location":"documentation/web_ingress/#configure-a-site","title":"Configure a Site","text":"<p>Each VM can support multiple sites - choose a unique key per site to ensure configuration is applied to the correct site. Keys must be unique within a VM. You should add a tag in the following format:</p> <pre><code>condenser_ingress_[site-key]_[label-name]: value\n</code></pre> <p>For example, if you choose a key, <code>test</code>, the <code>hostname</code> label would be configured using:</p> <pre><code>condenser_ingress_test_hostname: some-hostname-here\n</code></pre>"},{"location":"documentation/web_ingress/#required-labels","title":"Required Labels","text":"<ul> <li><code>condenser_ingress_[site-key]/hostname: [hostname]</code>: Used to determine the FQDN.   The final ingressed FQDN will be <code>[hostname].[rancher project name].condenser.arc.ucl.ac.uk</code></li> </ul>"},{"location":"documentation/web_ingress/#optional-labels","title":"Optional Labels","text":"<ul> <li><code>condenser_ingress_[site-key]/port: [port]</code>: Target port (default 443 if <code>protocol</code>   is https, 80 otherwise)</li> <li><code>condenser_ingress_[site-key]/protocol: [protocol]</code>: Target protocol (default http)</li> <li><code>condenser_ingress_[site-key]/vip: [vip]</code>: Target VIP (if the IP address is not   assigned to the VM)</li> <li><code>condenser_ingress_[site-key]/interface: [interface]</code>: Which network interface   to use if the VM has multiple network interfaces (default <code>eth0</code>)</li> </ul>"},{"location":"documentation/web_ingress/#advanced-nginx-configuration","title":"Advanced Nginx Configuration","text":"<p>In addition to basic ingress rules, all nginx annotations are supported.</p> <p>An annotation can be added to an ingress rule by substituting <code>nginx.ingress.kubernetes.io</code> with <code>condenser_ingress_[site-key]_nginx</code>. For example, to annotate an ingress rule, <code>test</code>, with <code>nginx.ingress.kubernetes.io/proxy-body-size: 8m</code>, add the following tag to your VM:</p> <ul> <li><code>condenser_ingress_test_nginx/proxy-body-size: 8m</code></li> </ul>"},{"location":"documentation/web_ingress/#rancher-gui","title":"Rancher GUI","text":"<p>To configure HTTPS ingress using the Rancher GUI, choose <code>Edit Config</code> on your VM and navigate to <code>Instance Labels</code>.</p> <p>Note: When saving your VM, Rancher will ask if you wish to restart the VM. Restarting the VM is not necessary to configure ingress.</p>"},{"location":"documentation/web_ingress/#enable-ingress-to-a-vm_1","title":"Enable Ingress to a VM","text":"<p>To enable a VM for ingress, add the Instance Label:</p> <ul> <li><code>condenser.ingress/isEnabled: true</code></li> </ul>"},{"location":"documentation/web_ingress/#configure-a-site_1","title":"Configure a Site","text":"<p>Each VM can support multiple sites - choose a unique key per site to ensure configuration is applied to the correct site. Keys must be unique within a VM. You should add a tag in the following format:</p> <ul> <li><code>condenser.ingress.[site-key]/[label-name]: value</code></li> </ul> <p>For example, if you choose a key, <code>test</code>, the <code>hostname</code> label would be configured using:</p> <ul> <li><code>condenser.ingress.test/hostname: some-hostname-here</code></li> </ul>"},{"location":"documentation/web_ingress/#required-labels_1","title":"Required Labels","text":"<ul> <li><code>condenser.ingress.[site-key]/hostname: &lt;hostname&gt;</code>: where the final ingressed   FQDN is <code>&lt;hostname&gt;.&lt;project name&gt;.condenser.arc.ucl.ac.uk</code></li> </ul>"},{"location":"documentation/web_ingress/#optional-labels_1","title":"Optional Labels","text":"<ul> <li><code>condenser.ingress.[site-key]/port: [port]</code>: Target port (default 443 if protocol   is https, 80 otherwise)</li> <li><code>condenser.ingress.[site-key]/protocol: [protocol]</code>: Target protocol (default http)</li> <li><code>condenser.ingress.[site-key]/vip: [vip]</code>: Target VIP (if the IP address is not   assigned to the VM)</li> <li><code>condenser.ingress.[site-key]/interface: [interface]</code>: Which network interface   to use if the VM has multiple network interfaces (default <code>eth0</code>)</li> </ul>"},{"location":"documentation/web_ingress/#advanced-configuration","title":"Advanced Configuration","text":"<p>In addition to basic ingress rules, all nginx annotations are supported.</p> <p>An annotation can be added to an ingress rule by substituting <code>nginx.ingress.kubernetes.io</code> with <code>condenser.ingress.[site-key].nginx</code>. For example, to annotate an ingress rule, <code>test</code>, with <code>nginx.ingress.kubernetes.io/proxy-body-size: 8m</code>, add the following instance label to your VM:</p> <ul> <li><code>condenser.ingress.test.nginx/proxy-body-size: 8m</code></li> </ul>"},{"location":"documentation/web_ingress/#examples","title":"Examples","text":""},{"location":"documentation/web_ingress/#basic-ingress","title":"Basic Ingress","text":"<p>Create an ingress, <code>test</code>, which proxies <code>test-host.&lt;project name&gt;.condenser.arc.ucl.ac.uk</code> to the VM on port 80:</p> <ul> <li><code>condenser.ingress/isEnabled: true</code></li> <li><code>condenser.ingress.test/hostname: test-host</code></li> </ul>"},{"location":"documentation/web_ingress/#basic-ingress-with-terraform","title":"Basic Ingress with Terraform","text":"<p>Create an ingress, <code>test</code>, which proxies <code>test-host.&lt;project name&gt;.condenser.arc.ucl.ac.uk</code> to the VM on port 80:</p> <pre><code>  tags = {\n    condenser_ingress_isEnabled = true\n    condenser_ingress_test_hostname = \"test-host\"\n  }\n</code></pre>"},{"location":"documentation/web_ingress/#advanced-ingress-with-terraform","title":"Advanced Ingress with Terraform","text":"<p>Create an ingress, <code>test</code>, which proxies <code>test-host.&lt;project name&gt;.condenser.arc.ucl.ac.uk</code> to the VM on port 80 with <code>proxy-body-size</code> set to 8m</p> <pre><code>  tags = {\n    condenser_ingress_isEnabled = true\n    condenser_ingress_test_hostname = \"test-host\"\n    condenser_ingress_test_nginx_proxy-body-size = \"8m\"\n  }\n</code></pre>"},{"location":"documentation/web_ingress/#https-ingress","title":"HTTPS Ingress","text":"<p>Create an ingress, <code>test</code>, which proxies <code>test-host.&lt;project name&gt;.condenser.arc.ucl.ac.uk</code> to the VM on port 443 using HTTPS:</p> <ul> <li><code>condenser.ingress/isEnabled: true</code></li> <li><code>condenser.ingress.test/hostname: test-host</code></li> <li><code>condenser.ingress.test/port: 443</code></li> <li><code>condenser.ingress.test/protocol: https</code></li> </ul>"},{"location":"documentation/web_ingress/#ingress-to-a-k3s-vip-on-a-custom-port","title":"Ingress to a K3s VIP on a custom port","text":"<p>Create an ingress, <code>testvip</code>, which proxies <code>test-host.&lt;project name&gt;.condenser.arc.ucl.ac.uk</code> to a K3s cluster's VIP, 10.134.8.9 on port 8080 using HTTP:</p> <ul> <li><code>condenser.ingress/isEnabled: true</code></li> <li><code>condenser.ingress.testvip/hostname: test-host</code></li> <li><code>condenser.ingress.testvip/port: 8080</code></li> <li><code>condenser.ingress.testvip/vip: 10.134.8.9</code></li> </ul>"},{"location":"documentation/web_ingress/#multiple-ingresses","title":"Multiple Ingresses","text":"<p>Create two ingresses, <code>testone</code> and <code>testtwo</code>, which proxy <code>testone.&lt;project name&gt;.condenser.arc.ucl.ac.uk</code> and <code>testtwo.&lt;project name&gt;.condenser.arc.ucl.ac.uk</code> to the VM on port 8080/8081 respectively using HTTP:</p> <ul> <li><code>condenser.ingress/isEnabled: true</code></li> <li><code>condenser.ingress.testone/hostname: testone</code></li> <li><code>condenser.ingress.testone/port: 8080</code></li> <li><code>condenser.ingress.testtwo/hostname: testtwo</code></li> <li><code>condenser.ingress.testtwo/port: 8081</code></li> </ul>"},{"location":"documentation/web_ingress/#multiple-ingresses-with-advanced-configuration","title":"Multiple Ingresses with advanced configuration","text":"<p>Create two ingresses, <code>testone</code> and <code>testtwo</code>, which proxy <code>testone.&lt;project name&gt;.condenser.arc.ucl.ac.uk</code> and <code>testtwo.&lt;project name&gt;.condenser.arc.ucl.ac.uk</code> to the VM on port 8080/8081 respectively using HTTP. <code>testone</code> requires a <code>proxy-buffer-size</code> of 8k, whilst <code>testtwo</code> needs a <code>proxy-body-size</code> of 8m:</p> <ul> <li><code>condenser.ingress/isEnabled: true</code></li> <li><code>condenser.ingress.testone/hostname: testone</code></li> <li><code>condenser.ingress.testone/port: 8080</code></li> <li><code>condenser.ingress.testone.nginx/proxy-buffer-size: 8k</code></li> <li><code>condenser.ingress.testtwo/hostname: testtwo</code></li> <li><code>condenser.ingress.testtwo/port: 8081</code></li> <li><code>condenser.ingress.testtwo.nginx/proxy-body-size: 8m</code></li> </ul>"},{"location":"documentation/deploying_resources/deploying_kubernetes/","title":"Deploying with kubectl","text":"<p>Resources on Condenser are typically deployed through Harvester onto an underlying kubernetes cluster. However, the cluster can be accessed directly using the kubectl tool. This can be used to access features that are not yet enabled by Harvester.</p> <p>On your local computer, install kubectl and obtain a kubeconfig file for a cluster on Condenser.</p> <p>Export the location of the kubeconfig file and try the following command to see the namespaces that you have permission to read:</p> <pre><code>export KUBECONFIG=/path/to/kubeconfig.yaml\nkubectl get ns\n</code></pre> <p>The output should be a similar table:</p> <pre><code>NAME                                     STATUS   AGE\narc-bespoke-ns                           Active   93d\narc-general-ns                           Active   204d\n</code></pre> <p>You can also use <code>kubectl apply</code> to deploy resources.</p> <p>Kubernetes deployments are described by yaml files. The following files describe a deployment like that created in the Rancher GUI and Terraform tutorials. In those tutorials, Harvester automatically deployed a volume to serve as the root block device for the VM. Here we will need to deploy it manually.</p>"},{"location":"documentation/deploying_resources/deploying_kubernetes/#describing-the-root-block-device","title":"Describing the root block device","text":"<p><code>pvc.yaml</code></p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: demo-kubectl-rootdisk\n  namespace: my-ns\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 30Gi\n  storageClassName: &lt;INSERT LONGHORN STORAGECLASS&gt;\n  volumeMode: Block\n</code></pre> <p>The highlighted lines indicate where you may need to alter the deployment. In particular, you need to use the storageClassName that corresponds to the VM image you want to use. You can discover this by first listing the images available to you:</p> <pre><code>$ kubectl get virtualmachineimage -A\nWarning: Use tokens from the TokenRequest API or manually created secret-based tokens instead of auto-generated secret-based tokens.\nNAMESPACE             NAME          DISPLAY-NAME                                SIZE         AGE\nharvester-public      image-mb9nd   almalinux-9.4-20240507                      602603520    107d\n</code></pre> <p>Then describe the image that you want to use and find the name of the corresponding storageClass:</p> <pre><code>$ kubectl describe virtualmachineimage --selector='harvesterhci.io/imageDisplayName=almalinux-9.4-20240507' -A\nWarning: Use tokens from the TokenRequest API or manually created secret-based tokens instead of auto-generated secret-based tokens.\nName:         image-mb9nd\nNamespace:    harvester-public\nLabels:       harvesterhci.io/image-type=raw_qcow2\n              harvesterhci.io/imageDisplayName=almalinux-9.4-20240507\n              harvesterhci.io/os-type=linux\nAnnotations:  harvesterhci.io/storageClassName: harvester-longhorn\nAPI Version:  harvesterhci.io/v1beta1\nKind:         VirtualMachineImage\n...\nStatus:\n  Applied URL:  https://repo.almalinux.org/almalinux/9/cloud/x86_64/images/AlmaLinux-9-GenericCloud-9.4-20240507.x86_64.qcow2\n  Conditions:\n    Last Update Time:  2024-11-04T18:17:02Z\n    Reason:            Imported\n    Status:            True\n    Type:              Imported\n    Last Update Time:  2024-11-04T18:16:44Z\n    Reason:            Initialized\n    Status:            True\n    Type:              Initialized\n  Failed:              0\n  Progress:            100\n  Size:                602603520\n  Storage Class Name:  longhorn-image-mb9nd\nEvents:                &lt;none&gt;\n</code></pre> <p>Or, like so:</p> <pre><code>$ kubectl get virtualmachineimage --selector='harvesterhci.io/imageDisplayName=almalinux-9.4-20240507' -A -o json | jq '.items[0].status.storageClassName'\nWarning: Use tokens from the TokenRequest API or manually created secret-based tokens instead of auto-generated secret-based tokens.\n\"longhorn-image-mb9nd\"\n</code></pre>"},{"location":"documentation/deploying_resources/deploying_kubernetes/#describing-the-vm","title":"Describing the VM","text":"<p><code>vm.yaml</code></p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: demo-kubectl\n  namespace: my-ns\nspec:\n  runStrategy: RerunOnFailure\n  template:\n    spec:\n      domain:\n        cpu:\n          cores: 2\n        devices:\n          disks:\n            - bootOrder: 1\n              disk:\n                bus: virtio\n              name: rootdisk\n            - disk:\n                bus: virtio\n              name: cloudinitdisk\n          interfaces:\n            - bridge: {}\n              model: virtio\n              name: nic-1\n        machine:\n          type: q35\n        memory:\n          guest: 16284Mi\n        resources:\n          limits:\n            cpu: '2'\n            memory: 16Gi\n          requests:\n            cpu: 125m\n            memory: 10922Mi\n      hostname: demo-kubectl\n      networks:\n        - multus:\n            networkName: my-ns/my-network\n          name: nic-1\n      volumes:\n        - name: rootdisk\n          persistentVolumeClaim:\n            claimName: demo-kubectl-rootdisk\n        - cloudInitNoCloud:\n            userData: |\n              #cloud-config\n              bootcmd:\n                - [ dnf, config-manager, --set-enabled, crb ]\n                - [ dnf, install, -y, epel-release ]\n\n              ssh_authorized_keys:\n                - &lt;INSERT SSH PUBLIC KEY DATA HERE&gt;\n          name: cloudinitdisk\n</code></pre> <p>The highlighted lines indicate where you may need to alter the deployment. In particular, you need to add your ssh public key to the cloudinit data. The claimName field needs to correspond to the name of the PersistentVolumeClaim that you use to provide the root block device.</p>"},{"location":"documentation/deploying_resources/deploying_kubernetes/#deploying","title":"Deploying","text":"<p>Create the files described above (<code>pvc.yaml</code> and <code>vm.yaml</code>). Then deploy these resources as follows:</p> <pre><code>kubectl apply -f pvc.yaml\nkubectl apply -f vm.yaml\n</code></pre> <p>Wait about 2 minutes for the IP address to become available:</p> <pre><code>$ kubectl get virtualmachineinstance --namespace my-ns\nWarning: Use tokens from the TokenRequest API or manually created secret-based tokens instead of auto-generated secret-based tokens.\nNAME           AGE    PHASE     IP            NODENAME          READY\ndemo-kubectl   58m    Running   10.134.X.X    harvester-2rbw7   True\n</code></pre> <p>Then you can log in, etc.</p>"},{"location":"documentation/deploying_resources/deploying_kubernetes/#destroying","title":"Destroying","text":"<p>First delete the VM, then the pvc:</p> <pre><code>kubectl delete -f vm.yaml\nkubectl delete -f pvc.yaml\n</code></pre>"},{"location":"documentation/deploying_resources/deploying_rancher/","title":"Deploying with the Rancher GUI","text":"<p>Condenser consists of several Harvester clusters. Harvester runs on Kubernetes; therefore each cluster is a specialized Kubernetes cluster. Rancher is used to control access to the clusters, and provides a GUI which is accessible within the UCL network.</p> <p>This tutorial demonstrates how to use the GUI to launch a VM on Condenser which is accessible by SSH.</p>"},{"location":"documentation/deploying_resources/deploying_rancher/#navigate-to-the-cluster-where-your-tenancy-is-assigned","title":"Navigate to the cluster where your tenancy is assigned","text":"<ol> <li>Enable the UCL VPN</li> <li>Navigate to the Rancher GUI for Condenser: <code>https://rancher.condenser.arc.ucl.ac.uk</code></li> <li>Click on Log in with AzureAD to log in with your UCL credentials</li> <li>Begin at the Home page: <code>https://rancher.condenser.arc.ucl.ac.uk/dashboard/home</code></li> <li>Open the navigation at left (\u2261) and click on Virtualization Management under Global Apps</li> <li>From the list of Harvester clusters, click on the name of the cluster you wish to access, e.g. <code>sl-p01</code></li> </ol>"},{"location":"documentation/deploying_resources/deploying_rancher/#register-your-ssh-key","title":"Register your SSH key","text":"<ol> <li>On your local computer, generate a SSH key pair if you do not already have one with a passphrase</li> <li>Copy the contents of your public key onto your clipboard</li> <li>In the Rancher GUI, from the menu at left, select Advanced &gt; SSH Keys</li> <li>On the SSH Keys page, click on Create in the upper right</li> <li>Select your tenancy namespace</li> <li>Enter an appropriate name for your SSH key</li> <li>Paste the contents of your public key into the text box under Basics</li> <li>Click Create in the lower right to register the SSH key</li> </ol>"},{"location":"documentation/deploying_resources/deploying_rancher/#launch-a-virtual-machine","title":"Launch a Virtual Machine","text":"<ol> <li>From the menu at left, select Virtual Machines</li> <li>Click Create in the upper right</li> <li>Select your tenancy namespace</li> <li>Name the VM (e.g., <code>demo</code>)</li> <li>In the VM submenu, select Networks</li> <li>From the Network drop-down menu, select your tenancy network</li> <li>In the VM submenu, select Volumes</li> <li>Select the appropriate image (e.g., <code>harvester-public/almalinux-9.4-20240805</code>)</li> <li>Increase the Size to 50 GB</li> <li>In the VM submenu, select Basics</li> <li>Increase the CPU setting to 2</li> <li>Increase the Memory to 8 GiB</li> <li>Select your SSH Key</li> <li> <p>Click Create in the lower right to launch the VM</p> <p>Note</p> <p>Occasionally the VM creation form will clear entries even though they appear to have been properly configured. This will cause errors to appear when you click Create. Re-enter them, wait for the Create button to reappear, and try again. It sometimes helps to focus the cursor out of the form fields after you configure a setting.</p> </li> </ol>"},{"location":"documentation/deploying_resources/deploying_rancher/#save-a-vm-configuration-as-a-template","title":"Save a VM configuration as a template","text":"<ol> <li>From the menu at left, select Virtual Machines</li> <li>Tick the box next to the VM</li> <li>From the menu at right (\u22ee), select Generate Template</li> <li>Name the template</li> <li>To launch a VM from a template, in the VM creation form tick the Use VM Template box and select the appropriate template</li> </ol>"},{"location":"documentation/deploying_resources/deploying_rancher/#log-in-to-the-vm-with-ssh","title":"Log in to the VM with SSH","text":"<ol> <li>From the menu at left, select Virtual Machines</li> <li>Identify the VM that you just launched in the list</li> <li>Wait for the IP address to appear, then copy it. The IP address will begin with <code>10.134</code>.</li> <li>Follow these instructions to access the VM with SSH. For VMs created from an Almalinux image, the username will be <code>almalinux</code>.</li> </ol>"},{"location":"documentation/deploying_resources/deploying_rancher/#change-a-vms-state","title":"Change a VM's state","text":"<ol> <li>From the menu at left, select Virtual Machines</li> <li>Tick the box next to the VM</li> <li>Click Stop, Restart, Start, or Delete at the top of the table of VMs</li> </ol>"},{"location":"documentation/deploying_resources/deploying_terraform/","title":"Deploying with Terraform","text":"<p>You can use the Terraform configuration language to automatically create and destroy virtual resources on Condenser. Hashicorp, the developer of Terraform, has produced extensive documentation and many tutorials for writing Terraform modules.</p> <p>Here we have created a short tutorial for writing and deploying a simple Terraform module that will use the Harvester Terraform provider to launch a small VM on Condenser. The VM is optionally configured for SSH access.</p>"},{"location":"documentation/deploying_resources/deploying_terraform/#preparation","title":"Preparation","text":"<ol> <li>Enable the UCL VPN</li> <li>Install the <code>terraform</code> and <code>kubectl</code> command line tools on your local computer</li> <li>Create the <code>kubeconfig.yaml</code> file</li> <li>Using the Rancher GUI, find the name of a namespace in which you have permission to deploy resources</li> <li>If you configure the VM for SSH access, you will also need:<ol> <li>The name of a VLAN network in the namespace</li> <li>The SSH public key data for your SSH key pair</li> </ol> </li> </ol>"},{"location":"documentation/deploying_resources/deploying_terraform/#write-a-basic-terraform-module-to-launch-a-vm","title":"Write a basic Terraform module to launch a VM","text":"<p>You may wish to start from the UCL-ARC/terraform-template, which provides a suggested file structure and a set of useful GitHub actions for Terraform projects.</p> <p>If you want to use the template, go to the repository page. In the upper right, click on \"Use this template\" and select \"Create a new repository\". After you have created the repository, clone it onto your local computer. In your terminal, navigate to the <code>terraform-template</code> directory. This will be the root directory of your terraform module.</p> <p>If you use the template, first delete the template content in each of the <code>*.tf</code> files.</p> <p>Alternatively, on your local computer create a new directory. This will be the root directory for your terraform module. In the new directory, create the following empty files:</p> <pre><code>main.tf\noutput.tf\nvariables.tf\nversions.tf\n</code></pre> <p>Copy the following content into the <code>versions.tf</code> file:</p> <pre><code>terraform {\n  required_providers {\n    harvester = {\n      source  = \"harvester/harvester\"\n      version = \"0.6.6\"\n    }\n  }\n\n  required_version = \"&gt;= 1.8.5\"\n}\n\nprovider \"harvester\" {}\n</code></pre> <p>Copy the following content into the <code>main.tf</code> file:</p> <pre><code>data \"harvester_image\" \"img\" {\n  display_name = \"almalinux-9.4-20240805\"\n  namespace    = \"harvester-public\"\n}\n\nresource \"harvester_virtualmachine\" \"vm\" {\n  name        = \"demo\"\n  namespace   = var.namespace\n  description = \"Demo VM\"\n  hostname    = \"demo\"\n\n  cpu    = 2\n  memory = \"8Gi\"\n\n  restart_after_update = true\n  efi                  = true\n  secure_boot          = true\n  run_strategy         = \"RerunOnFailure\"\n  reserved_memory      = \"100Mi\"\n  machine_type         = \"q35\"\n\n  disk {\n    name       = \"rootdisk\"\n    type       = \"disk\"\n    size       = \"50Gi\"\n    bus        = \"virtio\"\n    boot_order = 1\n\n    image       = data.harvester_image.img.id\n    auto_delete = true\n  }\n\n  network_interface {\n    name           = \"default\"\n  }\n}\n</code></pre> <p>The highlighted lines indicate where you can change the base image and other aspects of the VM.</p> <p>Copy the following content into the <code>variables.tf</code> file:</p> <pre><code>variable \"namespace\" {\n  type        = string\n  description = \"Namespace that the VM will be deployed in\"\n}\n</code></pre>"},{"location":"documentation/deploying_resources/deploying_terraform/#configure-the-vm-for-ssh-access","title":"Configure the VM for SSH access","text":"<p>These additions to the module are optional for deploying a VM, but are required to configure the VM to be accessible by SSH.</p> <p>Replace the <code>network_interface</code> block in the <code>harvester_virtualmachine.vm</code> resource in <code>main.tf</code> with the following:</p> <pre><code>  network_interface {\n    name           = \"nic-1\"\n    wait_for_lease = true\n    type           = \"bridge\"\n    network_name   = var.network_name\n  }\n</code></pre> <p>Add the following block to the <code>harvester_virtualmachine.vm</code> resource in <code>main.tf</code>:</p> <pre><code>  cloudinit {\n    user_data = &lt;&lt;EOF\n#cloud-config\npackage_update: true\npackages:\n  - qemu-guest-agent\nruncmd:\n  - - systemctl\n    - enable\n    - --now\n    - qemu-guest-agent.service\nssh_authorized_keys:\n  - ${var.ssh_public_key_data}\nEOF\n  }\n</code></pre> <p>Append the following content to the <code>variables.tf</code> file:</p> <pre><code>variable \"network_name\" {\n  type        = string\n  description = \"Name of a network in the namespace\"\n}\n\nvariable \"ssh_public_key_data\" {\n  type        = string\n  description = \"SSH public key data\"\n}\n</code></pre> <p>Copy the following content into the <code>output.tf</code> file:</p> <pre><code>output \"ip_address\" {\n  value = harvester_virtualmachine.vm.network_interface[0].ip_address\n}\n</code></pre>"},{"location":"documentation/deploying_resources/deploying_terraform/#deploy-the-terraform-module","title":"Deploy the Terraform module","text":"<p>Set the <code>KUBECONFIG</code> environment variable to point to your kubeconfig file:</p> <pre><code>export KUBECONFIG=\"/path/to/kubeconfig.yaml\"\n</code></pre> <p>From the root of your terraform module, run the following commands.</p> <pre><code>terraform init\nterraform apply\n</code></pre> <p>When prompted as below, enter the name of your namespace, and any other variables, and then <code>yes</code> to launch the VM.</p> <pre><code>...\nvar.namespace\n  Namespace that the VM will be deployed in\n\n  Enter a value:\n&gt; my-ns\n...\nPlan: 1 to add, 0 to change, 0 to destroy.\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value:\n&gt; yes\n</code></pre> <p>Note</p> <p>See the Terraform documentation to automatically configure input variables for your Terraform module.</p> <p>Terraform will report the following when the VM is deployed:</p> <pre><code>...\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n</code></pre> <p>You can monitor VMs that you launch with Terraform with the Rancher GUI or with <code>kubectl</code>:</p> <pre><code>kubectl get virtualmachineinstance.kubevirt.io --namespace my-ns\n</code></pre>"},{"location":"documentation/deploying_resources/deploying_terraform/#log-in-to-the-vm-with-ssh","title":"Log in to the VM with SSH","text":"<p>If you configured the VM for SSH access, the module will issue an IP address as output. Run <code>terraform output</code> or <code>terraform state show \"harvester_virtualmachine.vm\"</code> to print it again.</p> <p>For VMs created from an Almalinux image, the username will be <code>almalinux</code>.</p> <p>Follow these instructions to access the VM with SSH.</p>"},{"location":"documentation/deploying_resources/deploying_terraform/#destroy-the-vm","title":"Destroy the VM","text":"<p>When you are finished, run <code>terraform destroy</code> and answer <code>yes</code> when prompted to tear the VM down.</p>"},{"location":"documentation/deploying_resources/deploying_terraform/#troubleshooting","title":"Troubleshooting","text":""},{"location":"documentation/deploying_resources/deploying_terraform/#ip-address-is-not-reported-by-kubectl-or-the-rancher-gui","title":"IP address is not reported by <code>kubectl</code> or the Rancher GUI","text":"<p>If the IP address is successfully reported by Terraform, then the address has been assigned but the VM needs to be restarted before it will appear in the VM's attributes in the kubernetes cluster. You can restart a VM using the Rancher GUI, or by issuing the <code>reboot</code> command over SSH. You can configure the VM to reboot automatically after it launches by adding the following schema to the cloudinit <code>user_data</code>:</p> <pre><code>power_state:\n  mode: reboot\n</code></pre>"},{"location":"documentation/deploying_resources/deploying_terraform/#further-reading","title":"Further reading","text":"<ul> <li>Terraform configuration language documentation</li> <li>Harvester Terraform provider documentation</li> </ul>"},{"location":"stubs/kubeconfig/","title":"Obtaining a kubeconfig file","text":"<p>A kubeconfig file is required to remotely deploy infrastructure on the Harvester clusters within Condenser. You may need to provide it to <code>kubectl</code> or to the Harvester Terraform provider.</p> <p>The kubeconfig file contains a secret token that uses your credentials to authenticate to the Harvester cluster. Do not share it with anyone.</p> <ol> <li>Log in to the Rancher GUI</li> <li>Navigate to the Harvester cluster that you want to access</li> <li>In the bottom left corner, click on Support</li> <li>Click on Download KubeConfig. Your browser will download a file named <code>&lt;cluster name&gt;.yaml</code></li> <li>Open the downloaded file in a text editor</li> <li>Remove the <code>certificate-authority-data</code> key and value</li> </ol> <p>This is your kubeconfig file.</p> <p>The Harvester Terraform provider accepts the path to the kubeconfig file from the <code>KUBECONFIG</code> environment variable or as configuration to the provider block.</p> <pre><code>provider \"harvester\" {\n    kubeconfig = \"/path/to/kubeconfig.yaml\"\n}\n</code></pre> <p>See the Kubernetes documentation for instructions to use a kubeconfig file with <code>kubectl</code>.</p>"}]}